{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4580f384-63ea-44ec-bcde-c883db2346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import little_mallet_wrapper\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140a1962-6e8f-4038-9484-6f9ebcb6ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.read_csv('../articles.csv')\n",
    "\n",
    "# Convert dataframe to list\n",
    "original_texts = article_df['text'].tolist()\n",
    "titles = article_df['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9e38aa-8431-4894-b151-a70e84618cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "learning, network, policy, value, action, state, actions, learn, function, deep\n",
      "\n",
      "Topic 1\n",
      "image, images, features, network, deep, learning, feature, step, one, using\n",
      "\n",
      "Topic 2\n",
      "model, network, data, training, neural, function, code, time, layer, output\n",
      "\n",
      "Topic 3\n",
      "neural, data, learning, machine, network, part, use, example, learn, one\n",
      "\n",
      "Topic 4\n",
      "one, work, well, may, better, people, good, great, also, first\n",
      "\n",
      "Topic 5\n",
      "women, men, sex, attracted, blockchain, information, facial, face, lesbian, gay\n",
      "\n",
      "Topic 6\n",
      "like, people, would, time, get, really, going, want, one, even\n",
      "\n",
      "Topic 7\n",
      "human, intelligence, computer, google, artificial, humans, could, would, machine, machines\n",
      "\n",
      "Topic 8\n",
      "cnn, region, boxes, bounding, fast, mask, box, faster, segmentation, see\n",
      "\n",
      "Topic 9\n",
      "data, new, software, years, algorithms, companies, system, technology, many, world\n",
      "\n",
      "Topic 10\n",
      "cheat, sheet, bots, bot, https, data, facebook, online, chatbots, like\n",
      "\n",
      "Topic 11\n",
      "learning, machine, course, data, deep, python, science, reviews, hours, programming\n",
      "\n",
      "Topic 12\n",
      "gpu, gtx, learning, time, deep, gpus, run, cpu, power, also\n",
      "\n",
      "Topic 13\n",
      "human, game, systems, research, search, self, would, play, moves, man\n",
      "\n",
      "Topic 14\n",
      "word, words, vector, models, text, vectors, model, similar, rnn, matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list\n",
    "training_data = []\n",
    "\n",
    "# Process text using little mallet method - removes stop words and numbers\n",
    "for article in original_texts:\n",
    "    training_data.append(little_mallet_wrapper.process_string(article, numbers='remove'))\n",
    "\n",
    "# Define topics and words per topic\n",
    "num_topics = 15\n",
    "num_topic_words = 10\n",
    "\n",
    "\n",
    "# Initialize an LDA model with num_topics topics\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Clean and add tokens to model for training\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "\n",
    "# Train model\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "\n",
    "# Retrieve and display identified topics and topic words\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ', '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    print(f\"Topic {topic_number}\\n{topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f4b2c5-63b0-4a0d-b0b2-3bbb6ade731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "learning, network, policy, value, action, state, actions, learn, function, deep\n",
      "\n",
      "\n",
      "Topic Probability: 0.9423498511314392  \n",
      "Document: De la coopération entre les hommes et les machines, pour une approche pair-à-pair de l’intelligence...\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.8738723993301392  \n",
      "Document: Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "image, images, features, network, deep, learning, feature, step, one, using\n",
      "\n",
      "\n",
      "Topic Probability: 0.7309077978134155  \n",
      "Document: Intuitively Understanding Convolutions for Deep Learning\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.5768260359764099  \n",
      "Document: Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "model, network, data, training, neural, function, code, time, layer, output\n",
      "\n",
      "\n",
      "Topic Probability: 0.703730583190918  \n",
      "Document: A simple deep learning model for stock price prediction using TensorFlow\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.670115053653717  \n",
      "Document: Activation Functions: Neural Networks – Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.6460773944854736  \n",
      "Document: Yes you should understand backprop – Andrej Karpathy – Medium\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "neural, data, learning, machine, network, part, use, example, learn, one\n",
      "\n",
      "\n",
      "Topic Probability: 0.742332398891449  \n",
      "Document: Machine Learning is Fun! Part 2 – Adam Geitgey – Medium\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.7005326747894287  \n",
      "Document: Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "one, work, well, may, better, people, good, great, also, first\n",
      "\n",
      "\n",
      "Topic Probability: 0.5015115737915039  \n",
      "Document: How Airbnb uses Machine Learning to Detect Host Preferences\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.4232623279094696  \n",
      "Document: Why Data Scientists love Gaussian? – Towards Data Science\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.3818076252937317  \n",
      "Document: From Ballerina to AI Researcher: Part I – buZZrobot\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.3586283028125763  \n",
      "Document: Building a smarter home feed – Pinterest Engineering – Medium\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "women, men, sex, attracted, blockchain, information, facial, face, lesbian, gay\n",
      "\n",
      "\n",
      "Topic Probability: 0.8739655017852783  \n",
      "Document: ИИ-психопат и ИИ-обманщик – Hey Machine Learning\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.8405361771583557  \n",
      "Document: Do algorithms reveal sexual orientation or just expose our stereotypes?\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "like, people, would, time, get, really, going, want, one, even\n",
      "\n",
      "\n",
      "Topic Probability: 0.50180983543396  \n",
      "Document: What I learned from interviewing at multiple AI companies and start-ups\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.448182076215744  \n",
      "Document: Machine Learning: how to go from Zero to Hero – freeCodeCamp\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.4449355900287628  \n",
      "Document: Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "human, intelligence, computer, google, artificial, humans, could, would, machine, machines\n",
      "\n",
      "\n",
      "Topic Probability: 0.6189665794372559  \n",
      "Document: Emotional Computing – Robbie Tilton – Medium\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.4839342534542084  \n",
      "Document: Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.4778926968574524  \n",
      "Document: The mind-blowing AI announcement from Google that you probably missed.\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "cnn, region, boxes, bounding, fast, mask, box, faster, segmentation, see\n",
      "\n",
      "\n",
      "Topic Probability: 0.886609673500061  \n",
      "Document: Semántica desde información desestructurada – BEEVA Labs\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.7014135122299194  \n",
      "Document: A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "data, new, software, years, algorithms, companies, system, technology, many, world\n",
      "\n",
      "\n",
      "Topic Probability: 0.37695416808128357  \n",
      "Document: New to Machine Learning? Avoid these three mistakes\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.372042715549469  \n",
      "Document: Are Programmers Headed Toward Another Bursting Bubble?\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.32325634360313416  \n",
      "Document: The Evolving Role of Business Analytics – Frank Diana – Medium\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "cheat, sheet, bots, bot, https, data, facebook, online, chatbots, like\n",
      "\n",
      "\n",
      "Topic Probability: 0.9608274102210999  \n",
      "Document: Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.5310890674591064  \n",
      "Document: The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "learning, machine, course, data, deep, python, science, reviews, hours, programming\n",
      "\n",
      "\n",
      "Topic Probability: 0.9166969656944275  \n",
      "Document: Every single Machine Learning course on the internet, ranked by your reviews\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.5492696762084961  \n",
      "Document: Machine Learning เรียนอะไร, รู้ไปทําไม – O v e r f i t t e d – Medium\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "gpu, gtx, learning, time, deep, gpus, run, cpu, power, also\n",
      "\n",
      "\n",
      "Topic Probability: 0.8872417211532593  \n",
      "Document: The $1700 great Deep Learning box: Assembly, setup and benchmarks\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "human, game, systems, research, search, self, would, play, moves, man\n",
      "\n",
      "\n",
      "Topic Probability: 0.6737568974494934  \n",
      "Document: Why AI Research Loves Pac-Man – Tommy Thompson – Medium\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.5924811363220215  \n",
      "Document: Artificial Intelligence — The Revolution Hasn’t Happened Yet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve resulting distributions\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "# Iterate for display\n",
    "for topic_index in range(0,(num_topics - 1)):\n",
    "    # Sort based on probability\n",
    "    sorted_data = sorted([(_distribution[topic_index], _document) for _distribution, _document in zip(topic_distributions, titles)], reverse=True)\n",
    "\n",
    "    # Display identified topic words\n",
    "    topic_words = topics[topic_index]\n",
    "    print(f\"Topic {topic_index}:\\n{topic_words}\\n\")\n",
    "\n",
    "    # Record seen documents to remove repeats and display document and probability\n",
    "    seen_docs = []\n",
    "    for probability, doc in sorted_data[:5]:\n",
    "        if not doc in seen_docs:\n",
    "            print(f'\\nTopic Probability: {probability}  \\nDocument: {doc}\\n\\n')\n",
    "            seen_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a566a03-3233-4d62-89d4-98b9488fecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
