{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4580f384-63ea-44ec-bcde-c883db2346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import little_mallet_wrapper\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140a1962-6e8f-4038-9484-6f9ebcb6ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.read_csv('../articles.csv')\n",
    "\n",
    "# Convert dataframe to list\n",
    "original_texts = article_df['text'].tolist()\n",
    "titles = article_df['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9e38aa-8431-4894-b151-a70e84618cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "would, human, one, could, way, machine, like, system, future, learning, think, intelligence, world, make, something, new, google, even, game, really, research, language, going, take, artificial, learn, text, need, real, long\n",
      "\n",
      "Topic 1\n",
      "people, like, data, also, computer, new, time, work, many, companies, one, even, much, years, day, technology, well, user, great, use, might, information, product, would, app, questions, human, users, get, things\n",
      "\n",
      "Topic 2\n",
      "image, images, cnn, features, one, face, feature, different, object, process, step, see, deep, using, pixels, original, objects, computer, single, pixel, way, like, recognition, convolution, recognize, results, output, would, region, really\n",
      "\n",
      "Topic 3\n",
      "network, data, neural, learning, model, training, networks, function, time, use, deep, one, using, input, like, layer, example, code, much, get, also, first, set, see, output, two, need, used, train, number\n",
      "\n",
      "Topic 4\n",
      "learning, machine, data, course, python, science, free, hours, reviews, average, available, programming, cheat, rating, university, star, sheet, big, algorithms, https, weighted, weeks, learn, online, courses, week, videos, per, three, four\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list\n",
    "training_data = []\n",
    "\n",
    "# Process text using little mallet method - removes stop words and numbers\n",
    "for article in original_texts:\n",
    "    training_data.append(little_mallet_wrapper.process_string(article, numbers='remove'))\n",
    "\n",
    "# Define topics and words per topic\n",
    "num_topics = 5\n",
    "num_topic_words = 30\n",
    "\n",
    "\n",
    "# Initialize an LDA model with num_topics topics\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Clean and add tokens to model for training\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "\n",
    "# Train model\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "\n",
    "# Retrieve and display identified topics and topic words\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ', '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    print(f\"Topic {topic_number}\\n{topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f4b2c5-63b0-4a0d-b0b2-3bbb6ade731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "would, human, one, could, way, machine, like, system, future, learning, think, intelligence, world, make, something, new, google, even, game, really, research, language, going, take, artificial, learn, text, need, real, long\n",
      "\n",
      "\n",
      "Topic Probability: 0.7948070764541626  \n",
      "Document: The mind-blowing AI announcement from Google that you probably missed.\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.7661344408988953  \n",
      "Document: Why AI Research Loves Pac-Man – Tommy Thompson – Medium\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "people, like, data, also, computer, new, time, work, many, companies, one, even, much, years, day, technology, well, user, great, use, might, information, product, would, app, questions, human, users, get, things\n",
      "\n",
      "\n",
      "Topic Probability: 0.7496511340141296  \n",
      "Document: Do algorithms reveal sexual orientation or just expose our stereotypes?\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.6885490417480469  \n",
      "Document: Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "image, images, cnn, features, one, face, feature, different, object, process, step, see, deep, using, pixels, original, objects, computer, single, pixel, way, like, recognition, convolution, recognize, results, output, would, region, really\n",
      "\n",
      "\n",
      "Topic Probability: 0.9908145666122437  \n",
      "Document: A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "network, data, neural, learning, model, training, networks, function, time, use, deep, one, using, input, like, layer, example, code, much, get, also, first, set, see, output, two, need, used, train, number\n",
      "\n",
      "\n",
      "Topic Probability: 0.9059653282165527  \n",
      "Document: Yes you should understand backprop – Andrej Karpathy – Medium\n",
      "\n",
      "\n",
      "\n",
      "Topic Probability: 0.8717630505561829  \n",
      "Document: How to build a multi-layered neural network in Python\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve resulting distributions\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "# Iterate for display\n",
    "for topic_index in range(0,(num_topics - 1)):\n",
    "    # Sort based on probability\n",
    "    sorted_data = sorted([(_distribution[topic_index], _document) for _distribution, _document in zip(topic_distributions, titles)], reverse=True)\n",
    "\n",
    "    # Display identified topic words\n",
    "    topic_words = topics[topic_index]\n",
    "    print(f\"Topic {topic_index}:\\n{topic_words}\\n\")\n",
    "\n",
    "    # Record seen documents to remove repeats and display document and probability\n",
    "    seen_docs = []\n",
    "    for probability, doc in sorted_data[:5]:\n",
    "        if not doc in seen_docs:\n",
    "            print(f'\\nTopic Probability: {probability}  \\nDocument: {doc}\\n\\n')\n",
    "            seen_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a566a03-3233-4d62-89d4-98b9488fecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
